{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real-Time Japanese STT and Translation with MLX\n",
    "\n",
    "This notebook implements a real-time Japanese Speech-to-Text (STT) and Japanese-to-English translation pipeline using MLX on macOS, based on the provided report.\n",
    "\n",
    "**Requirements:**\n",
    "* macOS with Apple Silicon (M-series chip)\n",
    "* Python 3.8+\n",
    "* Required Python libraries: `mlx`, `mlx-lm`, `mlx-whisper`, `mlx-transformers`, `sounddevice`, `numpy`, `ipywidgets`, `transformers`, `sentencepiece`\n",
    "* System dependencies: `ffmpeg`, `portaudio` (install via Homebrew: `brew install ffmpeg portaudio`)\n",
    "* Microphone access permission for your terminal/Jupyter environment.\n",
    "\n",
    "**Instructions:**\n",
    "1.  Install all dependencies.\n",
    "2.  Run the cells sequentially.\n",
    "3.  Cell 2 loads the models (this might take time on the first run as models are downloaded).\n",
    "4.  Use the 'Start Listening' and 'Stop Listening' buttons in Cell 4 to control the pipeline.\n",
    "5.  Speak Japanese into your microphone after clicking 'Start Listening'.\n",
    "6.  Transcribed Japanese and translated English text will appear in the output area below the buttons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /opt/homebrew/lib/python3.10/site-packages (25.1.1)\n",
      "Requirement already satisfied: mlx in /opt/homebrew/lib/python3.10/site-packages (0.25.2)\n",
      "Requirement already satisfied: mlx-lm in /opt/homebrew/lib/python3.10/site-packages (0.24.0)\n",
      "Requirement already satisfied: mlx-whisper in /opt/homebrew/lib/python3.10/site-packages (0.4.2)\n",
      "Requirement already satisfied: mlx-transformers in /opt/homebrew/lib/python3.10/site-packages (0.1.5)\n",
      "Requirement already satisfied: sounddevice in /opt/homebrew/lib/python3.10/site-packages (0.5.1)\n",
      "Requirement already satisfied: numpy in /opt/homebrew/lib/python3.10/site-packages (1.26.4)\n",
      "Requirement already satisfied: ipywidgets in /opt/homebrew/lib/python3.10/site-packages (8.1.7)\n",
      "Requirement already satisfied: transformers in /opt/homebrew/lib/python3.10/site-packages (4.51.3)\n",
      "Requirement already satisfied: sentencepiece in /opt/homebrew/lib/python3.10/site-packages (0.2.0)\n",
      "Requirement already satisfied: protobuf in /opt/homebrew/lib/python3.10/site-packages (from mlx-lm) (4.25.7)\n",
      "Requirement already satisfied: pyyaml in /opt/homebrew/lib/python3.10/site-packages (from mlx-lm) (6.0.2)\n",
      "Requirement already satisfied: jinja2 in /opt/homebrew/lib/python3.10/site-packages (from mlx-lm) (3.1.6)\n",
      "Requirement already satisfied: numba in /opt/homebrew/lib/python3.10/site-packages (from mlx-whisper) (0.61.2)\n",
      "Requirement already satisfied: torch in /opt/homebrew/lib/python3.10/site-packages (from mlx-whisper) (2.7.0)\n",
      "Requirement already satisfied: tqdm in /opt/homebrew/lib/python3.10/site-packages (from mlx-whisper) (4.67.1)\n",
      "Requirement already satisfied: more-itertools in /opt/homebrew/lib/python3.10/site-packages (from mlx-whisper) (10.7.0)\n",
      "Requirement already satisfied: tiktoken in /opt/homebrew/lib/python3.10/site-packages (from mlx-whisper) (0.9.0)\n",
      "Requirement already satisfied: huggingface_hub in /opt/homebrew/lib/python3.10/site-packages (from mlx-whisper) (0.31.1)\n",
      "Requirement already satisfied: scipy in /opt/homebrew/lib/python3.10/site-packages (from mlx-whisper) (1.15.3)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/homebrew/lib/python3.10/site-packages (from mlx-transformers) (0.5.3)\n",
      "Requirement already satisfied: tokenizers>=0.19.1 in /opt/homebrew/lib/python3.10/site-packages (from mlx-transformers) (0.21.1)\n",
      "Requirement already satisfied: pillow==10.3.0 in /opt/homebrew/lib/python3.10/site-packages (from mlx-transformers) (10.3.0)\n",
      "Requirement already satisfied: streamlit==1.34.0 in /opt/homebrew/lib/python3.10/site-packages (from mlx-transformers) (1.34.0)\n",
      "Requirement already satisfied: altair<6,>=4.0 in /opt/homebrew/lib/python3.10/site-packages (from streamlit==1.34.0->mlx-transformers) (5.5.0)\n",
      "Requirement already satisfied: blinker<2,>=1.0.0 in /opt/homebrew/lib/python3.10/site-packages (from streamlit==1.34.0->mlx-transformers) (1.9.0)\n",
      "Requirement already satisfied: cachetools<6,>=4.0 in /opt/homebrew/lib/python3.10/site-packages (from streamlit==1.34.0->mlx-transformers) (5.5.2)\n",
      "Requirement already satisfied: click<9,>=7.0 in /opt/homebrew/lib/python3.10/site-packages (from streamlit==1.34.0->mlx-transformers) (8.2.0)\n",
      "Requirement already satisfied: packaging<25,>=16.8 in /opt/homebrew/lib/python3.10/site-packages (from streamlit==1.34.0->mlx-transformers) (24.2)\n",
      "Requirement already satisfied: pandas<3,>=1.3.0 in /opt/homebrew/lib/python3.10/site-packages (from streamlit==1.34.0->mlx-transformers) (2.2.3)\n",
      "Requirement already satisfied: pyarrow>=7.0 in /opt/homebrew/lib/python3.10/site-packages (from streamlit==1.34.0->mlx-transformers) (20.0.0)\n",
      "Requirement already satisfied: requests<3,>=2.27 in /opt/homebrew/lib/python3.10/site-packages (from streamlit==1.34.0->mlx-transformers) (2.32.3)\n",
      "Requirement already satisfied: rich<14,>=10.14.0 in /opt/homebrew/lib/python3.10/site-packages (from streamlit==1.34.0->mlx-transformers) (13.9.4)\n",
      "Requirement already satisfied: tenacity<9,>=8.1.0 in /opt/homebrew/lib/python3.10/site-packages (from streamlit==1.34.0->mlx-transformers) (8.5.0)\n",
      "Requirement already satisfied: toml<2,>=0.10.1 in /opt/homebrew/lib/python3.10/site-packages (from streamlit==1.34.0->mlx-transformers) (0.10.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.3.0 in /Users/katopz/Library/Python/3.10/lib/python/site-packages (from streamlit==1.34.0->mlx-transformers) (4.13.2)\n",
      "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /opt/homebrew/lib/python3.10/site-packages (from streamlit==1.34.0->mlx-transformers) (3.1.44)\n",
      "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /opt/homebrew/lib/python3.10/site-packages (from streamlit==1.34.0->mlx-transformers) (0.9.1)\n",
      "Requirement already satisfied: tornado<7,>=6.0.3 in /Users/katopz/Library/Python/3.10/lib/python/site-packages (from streamlit==1.34.0->mlx-transformers) (6.4.2)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /opt/homebrew/lib/python3.10/site-packages (from altair<6,>=4.0->streamlit==1.34.0->mlx-transformers) (4.23.0)\n",
      "Requirement already satisfied: narwhals>=1.14.2 in /opt/homebrew/lib/python3.10/site-packages (from altair<6,>=4.0->streamlit==1.34.0->mlx-transformers) (1.39.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/homebrew/lib/python3.10/site-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit==1.34.0->mlx-transformers) (4.0.12)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /opt/homebrew/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit==1.34.0->mlx-transformers) (5.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/katopz/Library/Python/3.10/lib/python/site-packages (from pandas<3,>=1.3.0->streamlit==1.34.0->mlx-transformers) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/homebrew/lib/python3.10/site-packages (from pandas<3,>=1.3.0->streamlit==1.34.0->mlx-transformers) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/homebrew/lib/python3.10/site-packages (from pandas<3,>=1.3.0->streamlit==1.34.0->mlx-transformers) (2025.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/lib/python3.10/site-packages (from requests<3,>=2.27->streamlit==1.34.0->mlx-transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/lib/python3.10/site-packages (from requests<3,>=2.27->streamlit==1.34.0->mlx-transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/lib/python3.10/site-packages (from requests<3,>=2.27->streamlit==1.34.0->mlx-transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/lib/python3.10/site-packages (from requests<3,>=2.27->streamlit==1.34.0->mlx-transformers) (2025.4.26)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/homebrew/lib/python3.10/site-packages (from rich<14,>=10.14.0->streamlit==1.34.0->mlx-transformers) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/katopz/Library/Python/3.10/lib/python/site-packages (from rich<14,>=10.14.0->streamlit==1.34.0->mlx-transformers) (2.19.1)\n",
      "Requirement already satisfied: CFFI>=1.0 in /opt/homebrew/lib/python3.10/site-packages (from sounddevice) (1.17.1)\n",
      "Requirement already satisfied: comm>=0.1.3 in /Users/katopz/Library/Python/3.10/lib/python/site-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /Users/katopz/Library/Python/3.10/lib/python/site-packages (from ipywidgets) (8.36.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /Users/katopz/Library/Python/3.10/lib/python/site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.14 in /opt/homebrew/lib/python3.10/site-packages (from ipywidgets) (4.0.14)\n",
      "Requirement already satisfied: jupyterlab_widgets~=3.0.15 in /opt/homebrew/lib/python3.10/site-packages (from ipywidgets) (3.0.15)\n",
      "Requirement already satisfied: filelock in /opt/homebrew/lib/python3.10/site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/homebrew/lib/python3.10/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/homebrew/lib/python3.10/site-packages (from huggingface_hub->mlx-whisper) (2025.3.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /opt/homebrew/lib/python3.10/site-packages (from huggingface_hub->mlx-whisper) (1.1.0)\n",
      "Requirement already satisfied: pycparser in /opt/homebrew/lib/python3.10/site-packages (from CFFI>=1.0->sounddevice) (2.22)\n",
      "Requirement already satisfied: decorator in /Users/katopz/Library/Python/3.10/lib/python/site-packages (from ipython>=6.1.0->ipywidgets) (5.2.1)\n",
      "Requirement already satisfied: exceptiongroup in /Users/katopz/Library/Python/3.10/lib/python/site-packages (from ipython>=6.1.0->ipywidgets) (1.3.0)\n",
      "Requirement already satisfied: jedi>=0.16 in /Users/katopz/Library/Python/3.10/lib/python/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in /Users/katopz/Library/Python/3.10/lib/python/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: pexpect>4.3 in /Users/katopz/Library/Python/3.10/lib/python/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /Users/katopz/Library/Python/3.10/lib/python/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.51)\n",
      "Requirement already satisfied: stack_data in /Users/katopz/Library/Python/3.10/lib/python/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: wcwidth in /Users/katopz/Library/Python/3.10/lib/python/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /Users/katopz/Library/Python/3.10/lib/python/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/homebrew/lib/python3.10/site-packages (from jinja2->mlx-lm) (3.0.2)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /opt/homebrew/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit==1.34.0->mlx-transformers) (25.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/homebrew/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit==1.34.0->mlx-transformers) (2025.4.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/homebrew/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit==1.34.0->mlx-transformers) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/homebrew/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit==1.34.0->mlx-transformers) (0.24.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/homebrew/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit==1.34.0->mlx-transformers) (0.1.2)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /Users/katopz/Library/Python/3.10/lib/python/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/katopz/Library/Python/3.10/lib/python/site-packages (from python-dateutil>=2.8.2->pandas<3,>=1.3.0->streamlit==1.34.0->mlx-transformers) (1.17.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/homebrew/lib/python3.10/site-packages (from torch->mlx-whisper) (1.14.0)\n",
      "Requirement already satisfied: networkx in /opt/homebrew/lib/python3.10/site-packages (from torch->mlx-whisper) (3.4.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/homebrew/lib/python3.10/site-packages (from sympy>=1.13.3->torch->mlx-whisper) (1.3.0)\n",
      "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in /opt/homebrew/lib/python3.10/site-packages (from numba->mlx-whisper) (0.44.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in /Users/katopz/Library/Python/3.10/lib/python/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (2.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /Users/katopz/Library/Python/3.10/lib/python/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in /Users/katopz/Library/Python/3.10/lib/python/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\u001b[34m==>\u001b[0m \u001b[1mDownloading https://formulae.brew.sh/api/formula.jws.json\u001b[0m\n",
      "\u001b[34m==>\u001b[0m \u001b[1mDownloading https://formulae.brew.sh/api/cask.jws.json\u001b[0m\n",
      "\u001b[33mWarning:\u001b[0m ffmpeg 7.1.1_2 is already installed and up-to-date.\n",
      "To reinstall 7.1.1_2, run:\n",
      "  brew reinstall ffmpeg\n",
      "\u001b[33mWarning:\u001b[0m portaudio 19.7.0 is already installed and up-to-date.\n",
      "To reinstall 19.7.0, run:\n",
      "  brew reinstall portaudio\n"
     ]
    }
   ],
   "source": [
    "!python3 -m venv .venv\n",
    "!source .venv/bin/activate\n",
    "!python3.10 -m pip install --upgrade pip\n",
    "%pip install mlx mlx-lm mlx-whisper mlx-transformers sounddevice numpy ipywidgets transformers sentencepiece\n",
    "!brew install ffmpeg portaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Could not import model classes from mlx_transformers.\n",
      "Translation functionality will be disabled or use placeholders.\n",
      "Please ensure mlx-transformers is installed correctly: pip install mlx-transformers\n",
      "Configuration loaded.\n",
      "STT Model: kaiinui/kotoba-whisper-v1.0-mlx\n",
      "Translation Model: facebook/nllb-200-distilled-600M\n",
      "Audio Sample Rate: 16000 Hz, Block Size: 1600 samples\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Imports and Global Configuration\n",
    "\n",
    "# Essential Imports\n",
    "import mlx.core as mx\n",
    "import mlx_whisper\n",
    "from transformers import AutoTokenizer, AutoConfig\n",
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "import queue\n",
    "import threading\n",
    "import time\n",
    "import sys\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "# Attempt to import mlx-transformers components\n",
    "# NOTE: The exact import path and class names might differ depending on the mlx-transformers library version.\n",
    "# Please check the mlx-transformers documentation/examples (e.g., nllb_translation.py) for the correct imports.\n",
    "try:\n",
    "    # This is a potential import path, adjust if necessary\n",
    "    from mlx_transformers.models.nllb import NLLBForConditionalGeneration as MLXNLLBModel\n",
    "    # If using M2M100, the import would be different, e.g.:\n",
    "    # from mlx_transformers.models.m2m100 import M2M100ForConditionalGeneration as MLXM2M100Model\n",
    "    print(\"Successfully imported MLXNLLBModel from mlx_transformers.\")\n",
    "    mlx_transformers_available = True\n",
    "except ImportError:\n",
    "    print(\"WARNING: Could not import model classes from mlx_transformers.\")\n",
    "    print(\"Translation functionality will be disabled or use placeholders.\")\n",
    "    print(\"Please ensure mlx-transformers is installed correctly: pip install mlx-transformers\")\n",
    "    MLXNLLBModel = None # Placeholder\n",
    "    # MLXM2M100Model = None # Placeholder\n",
    "    mlx_transformers_available = False\n",
    "\n",
    "# --- Configuration ---\n",
    "# Audio Parameters\n",
    "SAMPLE_RATE = 16000  # Hz (Required by Whisper)\n",
    "CHANNELS = 1         # Mono\n",
    "DTYPE = 'float32'    # Data type for audio samples\n",
    "BLOCK_DURATION_MS = 100 # Process audio in 100ms VAD chunks\n",
    "BLOCK_SIZE = int(SAMPLE_RATE * (BLOCK_DURATION_MS / 1000))\n",
    "\n",
    "# VAD Parameters (tune these based on your microphone and environment)\n",
    "VAD_THRESHOLD = 0.01          # Energy threshold for speech detection\n",
    "MIN_SPEECH_DURATION_MS = 250  # Minimum speech length to process (ms)\n",
    "SILENCE_DURATION_MS_TRIGGER = 700 # Silence after speech to trigger processing (ms)\n",
    "MAX_SPEECH_DURATION_MS = 15000 # Max duration before forcing processing (ms)\n",
    "\n",
    "min_speech_blocks = int(MIN_SPEECH_DURATION_MS / BLOCK_DURATION_MS)\n",
    "silence_blocks_trigger = int(SILENCE_DURATION_MS_TRIGGER / BLOCK_DURATION_MS)\n",
    "max_speech_blocks = int(MAX_SPEECH_DURATION_MS / BLOCK_DURATION_MS)\n",
    "\n",
    "# STT Model (Choose one)\n",
    "# STT_MODEL_NAME = \"mlx-community/whisper-tiny-mlx\" # Faster, less accurate\n",
    "# STT_MODEL_NAME = \"mlx-community/whisper-base-mlx\"\n",
    "# STT_MODEL_NAME = \"mlx-community/whisper-small-mlx\"\n",
    "# STT_MODEL_NAME = \"mlx-community/whisper-medium-mlx\"\n",
    "STT_MODEL_NAME = \"kaiinui/kotoba-whisper-v1.0-mlx\" # Japanese-optimized, based on distil-large-v3\n",
    "# STT_MODEL_NAME = \"mlx-community/whisper-large-v3-mlx\" # Most accurate, slowest\n",
    "\n",
    "# Translation Model (Using NLLB as example)\n",
    "TRANSLATION_MODEL_HF_ID = \"facebook/nllb-200-distilled-600M\"\n",
    "# Alternative: TRANSLATION_MODEL_HF_ID = \"facebook/m2m100_418M\"\n",
    "# Ensure the corresponding MLX model class (MLXNLLBModel or MLXM2M100Model) is imported above.\n",
    "\n",
    "# --- Global Variables ---\n",
    "audio_queue = queue.Queue()\n",
    "stt_model_loaded = None\n",
    "translator_model_loaded = None\n",
    "translator_tokenizer_loaded = None\n",
    "processing_thread = None\n",
    "stop_event = threading.Event() # Used to signal the thread to stop\n",
    "audio_stream = None\n",
    "\n",
    "print(\"Configuration loaded.\")\n",
    "print(f\"STT Model: {STT_MODEL_NAME}\")\n",
    "print(f\"Translation Model: {TRANSLATION_MODEL_HF_ID}\")\n",
    "print(f\"Audio Sample Rate: {SAMPLE_RATE} Hz, Block Size: {BLOCK_SIZE} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading STT model: kaiinui/kotoba-whisper-v1.0-mlx...\n",
      "STT model 'kaiinui/kotoba-whisper-v1.0-mlx' ready.\n",
      "Skipping translation model loading as mlx-transformers components are not available.\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Model Loading Functions\n",
    "\n",
    "def load_stt_model():\n",
    "    \"\"\"Loads the specified STT model (or confirms it's ready).\"\"\"\n",
    "    global stt_model_loaded\n",
    "    print(f\"Loading STT model: {STT_MODEL_NAME}...\")\n",
    "    # mlx_whisper typically loads the model on the first transcribe call.\n",
    "    # We can do a dummy transcribe to trigger download/initial load if needed.\n",
    "    try:\n",
    "        # Create a tiny silent audio segment\n",
    "        dummy_audio = np.zeros(int(SAMPLE_RATE * 0.1), dtype=np.float32) # 0.1 seconds\n",
    "        # Perform a dummy transcription\n",
    "        mlx_whisper.transcribe(dummy_audio, path_or_hf_repo=STT_MODEL_NAME, language=\"ja\")\n",
    "        stt_model_loaded = STT_MODEL_NAME # Mark as ready\n",
    "        print(f\"STT model '{STT_MODEL_NAME}' ready.\")\n",
    "        mx.eval() # Evaluate any potential lazy loading operations\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Could not initialize STT model ({STT_MODEL_NAME}): {e}\", file=sys.stderr)\n",
    "        stt_model_loaded = None\n",
    "\n",
    "def load_translation_model():\n",
    "    \"\"\"Loads the specified translation model and tokenizer using mlx-transformers.\"\"\"\n",
    "    global translator_model_loaded, translator_tokenizer_loaded\n",
    "\n",
    "    if not mlx_transformers_available:\n",
    "        print(\"Skipping translation model loading as mlx-transformers components are not available.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Loading Translation model: {TRANSLATION_MODEL_HF_ID}...\")\n",
    "    try:\n",
    "        # Load tokenizer using Hugging Face transformers\n",
    "        # For NLLB, specify source language\n",
    "        if \"nllb\" in TRANSLATION_MODEL_HF_ID.lower():\n",
    "            translator_tokenizer_loaded = AutoTokenizer.from_pretrained(TRANSLATION_MODEL_HF_ID, src_lang=\"jpn_Jpan\")\n",
    "            print(\"Loaded NLLB tokenizer.\")\n",
    "            ModelClass = MLXNLLBModel\n",
    "        # For M2M100, set source language after loading\n",
    "        elif \"m2m100\" in TRANSLATION_MODEL_HF_ID.lower():\n",
    "            translator_tokenizer_loaded = AutoTokenizer.from_pretrained(TRANSLATION_MODEL_HF_ID)\n",
    "            translator_tokenizer_loaded.src_lang = \"ja\"\n",
    "            print(\"Loaded M2M100 tokenizer.\")\n",
    "            # Ensure MLXM2M100Model was imported correctly if using M2M100\n",
    "            # ModelClass = MLXM2M100Model\n",
    "            # For this example, we assume NLLB is used, so ModelClass is MLXNLLBModel\n",
    "            # If you switch to M2M100, make sure the correct class is imported and assigned here.\n",
    "            print(\"WARNING: M2M100 model class not fully configured in this example. Assuming NLLB structure.\")\n",
    "            ModelClass = MLXNLLBModel # Defaulting to NLLB structure for the example flow\n",
    "        else:\n",
    "            print(f\"WARNING: Model type for {TRANSLATION_MODEL_HF_ID} not explicitly handled (NLLB/M2M100). Attempting generic load.\")\n",
    "            translator_tokenizer_loaded = AutoTokenizer.from_pretrained(TRANSLATION_MODEL_HF_ID)\n",
    "            # Assuming NLLB structure if type is unknown\n",
    "            ModelClass = MLXNLLBModel\n",
    "\n",
    "        # Load model configuration\n",
    "        config = AutoConfig.from_pretrained(TRANSLATION_MODEL_HF_ID)\n",
    "\n",
    "        # Instantiate the MLX model class\n",
    "        # NOTE: This assumes the imported ModelClass (e.g., MLXNLLBModel) is correct\n",
    "        # and follows the pattern of taking config during instantiation.\n",
    "        if ModelClass:\n",
    "             # The from_pretrained method might handle both instantiation and weight loading in mlx-transformers\n",
    "             # Check the library's specific API. Using a common pattern here:\n",
    "            try:\n",
    "                 # Try loading directly with from_pretrained class method if available\n",
    "                 translator_model_loaded = ModelClass.from_pretrained(TRANSLATION_MODEL_HF_ID)\n",
    "                 print(f\"Loaded translation model weights using {ModelClass.__name__}.from_pretrained.\")\n",
    "            except AttributeError:\n",
    "                 # Fallback: Instantiate then load weights (less common for HF-like APIs)\n",
    "                 translator_model_loaded = ModelClass(config)\n",
    "                 # Assuming a load_weights or similar method if from_pretrained isn't a class method\n",
    "                 # This part is highly dependent on mlx-transformers API design\n",
    "                 # translator_model_loaded.load_weights(TRANSLATION_MODEL_HF_ID) # Placeholder concept\n",
    "                 print(f\"Instantiated {ModelClass.__name__}. Weight loading mechanism needs verification for mlx-transformers.\")\n",
    "                 # For now, let's assume the first from_pretrained worked or mark as potentially unloaded\n",
    "                 # If the first try failed and this path is taken, it likely means the model isn't fully loaded.\n",
    "                 print(\"WARNING: Translation model might not be fully loaded due to API uncertainty.\")\n",
    "        else:\n",
    "            print(\"ERROR: MLX Model class not available. Cannot load translation model.\")\n",
    "            translator_model_loaded = None\n",
    "            translator_tokenizer_loaded = None\n",
    "            return\n",
    "\n",
    "        # Ensure model parameters are evaluated by MLX\n",
    "        if translator_model_loaded and hasattr(translator_model_loaded, 'parameters'):\n",
    "             mx.eval(translator_model_loaded.parameters())\n",
    "             print(f\"Translation model '{TRANSLATION_MODEL_HF_ID}' ready.\")\n",
    "        elif translator_model_loaded:\n",
    "             print(f\"Translation model '{TRANSLATION_MODEL_HF_ID}' loaded, but parameter evaluation skipped (no parameters attribute found).\")\n",
    "        else:\n",
    "             print(f\"Translation model '{TRANSLATION_MODEL_HF_ID}' failed to load.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Could not load translation model ({TRANSLATION_MODEL_HF_ID}): {e}\", file=sys.stderr)\n",
    "        translator_model_loaded = None\n",
    "        translator_tokenizer_loaded = None\n",
    "\n",
    "# --- Execute Loading ---\n",
    "load_stt_model()\n",
    "load_translation_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Audio Callback and Processing Functions\n",
    "\n",
    "def audio_callback(indata, frames, time, status):\n",
    "    \"\"\"This is called (from a separate thread) for each audio block.\"\"\"\n",
    "    if status:\n",
    "        print(status, file=sys.stderr) # Print errors to stderr\n",
    "    # Add the audio data (NumPy array) to the queue\n",
    "    audio_queue.put(indata.copy())\n",
    "\n",
    "def process_stt(audio_segment_np):\n",
    "    \"\"\"Performs STT on a NumPy audio segment using mlx-whisper.\"\"\"\n",
    "    if not stt_model_loaded or audio_segment_np.size == 0:\n",
    "        return \"\"\n",
    "    try:\n",
    "        # Ensure audio is float32. sounddevice callback provides float32 by default with dtype='float32'.\n",
    "        # Normalization is handled internally by mlx-whisper if needed.\n",
    "        result = mlx_whisper.transcribe(\n",
    "            audio_segment_np,\n",
    "            path_or_hf_repo=stt_model_loaded,\n",
    "            language=\"ja\", # Specify Japanese\n",
    "            # verbose=False # Set to True for more detailed whisper output\n",
    "        )\n",
    "        mx.eval() # Ensure transcription is computed\n",
    "        return result[\"text\"].strip()\n",
    "    except Exception as e:\n",
    "        print(f\"STT Error: {e}\", file=sys.stderr)\n",
    "        return \"\"\n",
    "\n",
    "def process_translation(japanese_text):\n",
    "    \"\"\"Translates Japanese text to English using the loaded MLX NMT model.\"\"\"\n",
    "    if not translator_model_loaded or not translator_tokenizer_loaded or not japanese_text:\n",
    "        return \"(Translation model not loaded or no input)\"\n",
    "\n",
    "    try:\n",
    "        # Tokenize the Japanese text\n",
    "        # Ensure return_tensors='np' is not used, mlx usually expects python lists/ints for token ids\n",
    "        # The mlx-transformers API might expect mx.array directly, check documentation.\n",
    "        # Using Hugging Face tokenizer standard practice first:\n",
    "        inputs = translator_tokenizer_loaded(japanese_text, return_tensors=\"pt\", padding=True, truncation=True) # Using 'pt' for PyTorch tensors first\n",
    "\n",
    "        # Convert inputs to mx.array\n",
    "        # NOTE: This conversion step might be handled differently or automatically by mlx-transformers.\n",
    "        # Check the library's expected input format for generate().\n",
    "        input_ids = mx.array(inputs.input_ids.numpy()) # Convert from PyTorch tensor to numpy then to mx.array\n",
    "        attention_mask = mx.array(inputs.attention_mask.numpy())\n",
    "\n",
    "        # Determine the forced beginning-of-sentence token ID for the target language (English)\n",
    "        forced_bos_token_id = None\n",
    "        if \"nllb\" in TRANSLATION_MODEL_HF_ID.lower():\n",
    "            # NLLB uses language codes like 'eng_Latn'\n",
    "            try:\n",
    "                forced_bos_token_id = translator_tokenizer_loaded.lang_code_to_id[\"eng_Latn\"]\n",
    "            except KeyError:\n",
    "                print(\"Warning: Could not find 'eng_Latn' in NLLB tokenizer. Translation might fail.\", file=sys.stderr)\n",
    "        elif \"m2m100\" in TRANSLATION_MODEL_HF_ID.lower():\n",
    "            # M2M100 uses language codes like 'en'\n",
    "            try:\n",
    "                forced_bos_token_id = translator_tokenizer_loaded.get_lang_id(\"en\")\n",
    "            except Exception:\n",
    "                 print(\"Warning: Could not get 'en' lang ID from M2M100 tokenizer. Translation might fail.\", file=sys.stderr)\n",
    "\n",
    "        if forced_bos_token_id is None:\n",
    "            print(\"Warning: Forced BOS token ID for English not set. Translation might be incorrect.\", file=sys.stderr)\n",
    "\n",
    "        # Generate translation using the model's generate method\n",
    "        # NOTE: The exact arguments for generate() depend heavily on the mlx-transformers implementation.\n",
    "        # Common arguments include input_ids, attention_mask, and forced_bos_token_id.\n",
    "        # Check the mlx-transformers examples (e.g., nllb_translation.py).\n",
    "        output_tokens = translator_model_loaded.generate(\n",
    "            input_ids,\n",
    "            # attention_mask=attention_mask, # Include if required by the specific model/implementation\n",
    "            forced_bos_token_id=forced_bos_token_id,\n",
    "            # max_length=100, # Optional: limit output length\n",
    "        )\n",
    "\n",
    "        mx.eval(output_tokens) # Ensure generation is computed\n",
    "\n",
    "        # Decode the generated token IDs\n",
    "        # The output_tokens might be an mx.array or list of lists.\n",
    "        if isinstance(output_tokens, mx.array):\n",
    "            # Assuming batch size 1, get the first element if it's a batch\n",
    "            if output_tokens.ndim > 1:\n",
    "                 tokens_to_decode = output_tokens[0].tolist()\n",
    "            else:\n",
    "                 tokens_to_decode = output_tokens.tolist()\n",
    "        elif isinstance(output_tokens, list) and len(output_tokens) > 0 and isinstance(output_tokens[0], list):\n",
    "             tokens_to_decode = output_tokens[0] # Assuming batch size 1\n",
    "        else:\n",
    "             tokens_to_decode = output_tokens # Assume it's already a flat list of token IDs\n",
    "\n",
    "        # Use batch_decode for robustness, even with a single sequence\n",
    "        english_text = translator_tokenizer_loaded.batch_decode([tokens_to_decode], skip_special_tokens=True)\n",
    "\n",
    "        # batch_decode returns a list of strings, get the first one\n",
    "        return english_text[0].strip() if english_text else \"(Empty translation)\"\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Translation Error: {e}\", file=sys.stderr)\n",
    "        # Provide more context about the error if possible\n",
    "        import traceback\n",
    "        print(traceback.format_exc(), file=sys.stderr)\n",
    "        return \"(Translation failed)\"\n",
    "\n",
    "def processing_loop(output_widget):\n",
    "    \"\"\"The main loop to process audio chunks from the queue.\"\"\"\n",
    "    global stop_event\n",
    "\n",
    "    # VAD state variables\n",
    "    speech_buffer_list = []\n",
    "    current_speech_duration_blocks = 0\n",
    "    silent_after_speech_blocks = 0\n",
    "    is_currently_speaking = False\n",
    "\n",
    "    with output_widget:\n",
    "        print(\"Processing thread started. Listening...\")\n",
    "\n",
    "    while not stop_event.is_set():\n",
    "        try:\n",
    "            # Get audio chunk from the queue\n",
    "            audio_chunk = audio_queue.get(block=True, timeout=0.1) # Timeout allows checking stop_event\n",
    "\n",
    "            # Simple VAD: Calculate RMS energy of the chunk\n",
    "            chunk_energy = np.sqrt(np.mean(audio_chunk**2))\n",
    "\n",
    "            # --- VAD Logic ---\n",
    "            if chunk_energy > VAD_THRESHOLD:\n",
    "                # Speech detected\n",
    "                if not is_currently_speaking:\n",
    "                     with output_widget:\n",
    "                         # clear_output(wait=True) # Optional: clear previous status messages\n",
    "                         print(\"Speech detected...\")\n",
    "                is_currently_speaking = True\n",
    "                speech_buffer_list.append(audio_chunk)\n",
    "                current_speech_duration_blocks += 1\n",
    "                silent_after_speech_blocks = 0\n",
    "\n",
    "                # Check if speech duration exceeds maximum\n",
    "                process_now = current_speech_duration_blocks >= max_speech_blocks\n",
    "\n",
    "            elif is_currently_speaking:\n",
    "                # Silence detected after speech\n",
    "                speech_buffer_list.append(audio_chunk) # Append silence chunk for trailing context\n",
    "                silent_after_speech_blocks += 1\n",
    "                process_now = silent_after_speech_blocks >= silence_blocks_trigger\n",
    "            else:\n",
    "                # Silence while not speaking\n",
    "                process_now = False\n",
    "\n",
    "            # --- Processing Trigger ---\n",
    "            if process_now and len(speech_buffer_list) >= min_speech_blocks:\n",
    "                full_speech_segment = np.concatenate(speech_buffer_list)\n",
    "                segment_duration = len(full_speech_segment) / SAMPLE_RATE\n",
    "\n",
    "                with output_widget:\n",
    "                    clear_output(wait=True) # Clear previous output for cleaner display\n",
    "                    print(f\"Processing {segment_duration:.2f}s audio segment...\")\n",
    "\n",
    "                # 1. Perform STT\n",
    "                jp_text = process_stt(full_speech_segment)\n",
    "\n",
    "                # 2. Perform Translation (if STT was successful)\n",
    "                en_text = \"\"\n",
    "                if jp_text:\n",
    "                    en_text = process_translation(jp_text)\n",
    "\n",
    "                # 3. Display Results\n",
    "                with output_widget:\n",
    "                    clear_output(wait=True) # Clear processing message\n",
    "                    if jp_text:\n",
    "                        print(f\"🇯🇵: {jp_text}\")\n",
    "                        if en_text:\n",
    "                             print(f\"🇬🇧: {en_text}\")\n",
    "                        else:\n",
    "                             print(\"🇬🇧: (Translation failed or disabled)\")\n",
    "                    else:\n",
    "                        print(\"(No speech detected or STT failed)\")\n",
    "                    print(\"\\nListening...\") # Indicate readiness for next utterance\n",
    "\n",
    "                # Reset VAD state after processing\n",
    "                speech_buffer_list = []\n",
    "                current_speech_duration_blocks = 0\n",
    "                silent_after_speech_blocks = 0\n",
    "                is_currently_speaking = False\n",
    "\n",
    "            elif process_now: # Process triggered but speech too short\n",
    "                 # Reset VAD state without processing\n",
    "                speech_buffer_list = []\n",
    "                current_speech_duration_blocks = 0\n",
    "                silent_after_speech_blocks = 0\n",
    "                is_currently_speaking = False\n",
    "                with output_widget:\n",
    "                     clear_output(wait=True)\n",
    "                     print(\"(Speech too short, ignored)\\nListening...\")\n",
    "\n",
    "        except queue.Empty:\n",
    "            # Queue timeout - allows checking stop_event periodically\n",
    "            # Check if we were speaking and silence duration is met due to timeout\n",
    "            if is_currently_speaking and len(speech_buffer_list) >= min_speech_blocks:\n",
    "                 silent_after_speech_blocks += 1 # Count timeout as silence\n",
    "                 if silent_after_speech_blocks >= silence_blocks_trigger:\n",
    "                      # Process accumulated speech due to timeout silence\n",
    "                     full_speech_segment = np.concatenate(speech_buffer_list)\n",
    "                     segment_duration = len(full_speech_segment) / SAMPLE_RATE\n",
    "                     with output_widget:\n",
    "                          clear_output(wait=True)\n",
    "                          print(f\"Processing {segment_duration:.2f}s audio segment (timeout)...\")\n",
    "\n",
    "                     jp_text = process_stt(full_speech_segment)\n",
    "                     en_text = \"\"\n",
    "                     if jp_text:\n",
    "                          en_text = process_translation(jp_text)\n",
    "\n",
    "                     with output_widget:\n",
    "                          clear_output(wait=True)\n",
    "                          if jp_text:\n",
    "                               print(f\"🇯🇵: {jp_text}\")\n",
    "                               if en_text:\n",
    "                                    print(f\"🇬🇧: {en_text}\")\n",
    "                               else:\n",
    "                                    print(\"🇬🇧: (Translation failed or disabled)\")\n",
    "                          else:\n",
    "                               print(\"(No speech detected or STT failed)\")\n",
    "                          print(\"\\nListening...\")\n",
    "\n",
    "                     # Reset VAD state\n",
    "                     speech_buffer_list = []\n",
    "                     current_speech_duration_blocks = 0\n",
    "                     silent_after_speech_blocks = 0\n",
    "                     is_currently_speaking = False\n",
    "            continue # Continue loop after timeout\n",
    "\n",
    "        except Exception as e:\n",
    "            with output_widget:\n",
    "                clear_output(wait=True)\n",
    "                print(f\"\\nError in processing loop: {e}\", file=sys.stderr)\n",
    "                import traceback\n",
    "                print(traceback.format_exc(), file=sys.stderr)\n",
    "            # Optionally break or try to recover\n",
    "            time.sleep(1) # Avoid busy-looping on error\n",
    "\n",
    "    # Loop exited (stop_event was set)\n",
    "    with output_widget:\n",
    "        clear_output(wait=True)\n",
    "        print(\"Processing thread stopped.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f45f4e7e5534551ba84f7b5021634d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Button(button_style='success', description='Start Listening', icon='microphone', style=ButtonSt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3ba59a1f258443ca25cb7162146e783",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Label(value='Status: Idle')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f5cdf1cf7aa444ea8218c4b51c6d524",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output(layout=Layout(border_bottom='1px solid black', border_left='1px solid black', border_right='1px solid b…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cell 4: UI Elements and Control Logic\n",
    "\n",
    "# Create Widgets\n",
    "start_button = widgets.Button(description=\"Start Listening\", button_style='success', icon='microphone')\n",
    "stop_button = widgets.Button(description=\"Stop Listening\", button_style='danger', icon='stop', disabled=True)\n",
    "output_area = widgets.Output(layout={'border': '1px solid black', 'height': '300px', 'overflow_y': 'scroll'})\n",
    "status_label = widgets.Label(value=\"Status: Idle\")\n",
    "\n",
    "def start_listening(b):\n",
    "    \"\"\"Callback function for the Start button.\"\"\"\n",
    "    global processing_thread, stop_event, audio_stream\n",
    "\n",
    "    if not stt_model_loaded:\n",
    "         with output_area:\n",
    "             clear_output(wait=True)\n",
    "             print(\"ERROR: STT Model not loaded. Cannot start.\")\n",
    "         return\n",
    "    # Optional: Check if translation model loaded if it's critical\n",
    "    # if not translator_model_loaded and mlx_transformers_available:\n",
    "    #     with output_area:\n",
    "    #         clear_output(wait=True)\n",
    "    #         print(\"WARNING: Translation Model not loaded. Proceeding with STT only.\")\n",
    "\n",
    "    start_button.disabled = True\n",
    "    stop_button.disabled = False\n",
    "    status_label.value = \"Status: Initializing...\"\n",
    "    output_area.clear_output()\n",
    "\n",
    "    # Clear the queue\n",
    "    while not audio_queue.empty():\n",
    "        try:\n",
    "            audio_queue.get_nowait()\n",
    "        except queue.Empty:\n",
    "            break\n",
    "\n",
    "    # Reset stop event\n",
    "    stop_event.clear()\n",
    "\n",
    "    # Start the audio stream\n",
    "    try:\n",
    "        # Check available devices (optional, for debugging)\n",
    "        # print(sd.query_devices())\n",
    "        audio_stream = sd.InputStream(\n",
    "            samplerate=SAMPLE_RATE,\n",
    "            blocksize=BLOCK_SIZE,\n",
    "            channels=CHANNELS,\n",
    "            dtype=DTYPE,\n",
    "            callback=audio_callback\n",
    "        )\n",
    "        audio_stream.start()\n",
    "        status_label.value = \"Status: Listening...\"\n",
    "        with output_area:\n",
    "             print(\"Audio stream started. Speak Japanese.\")\n",
    "\n",
    "        # Start the processing thread\n",
    "        processing_thread = threading.Thread(target=processing_loop, args=(output_area,))\n",
    "        processing_thread.start()\n",
    "\n",
    "    except Exception as e:\n",
    "        status_label.value = \"Status: Error starting stream!\"\n",
    "        with output_area:\n",
    "             print(f\"Error starting audio stream: {e}\", file=sys.stderr)\n",
    "             import traceback\n",
    "             print(traceback.format_exc(), file=sys.stderr)\n",
    "        start_button.disabled = False\n",
    "        stop_button.disabled = True\n",
    "        if audio_stream:\n",
    "             try:\n",
    "                 if audio_stream.active:\n",
    "                     audio_stream.stop()\n",
    "                 audio_stream.close()\n",
    "             except Exception as close_e:\n",
    "                  print(f\"Error closing stream after start failure: {close_e}\", file=sys.stderr)\n",
    "             audio_stream = None\n",
    "\n",
    "def stop_listening(b):\n",
    "    \"\"\"Callback function for the Stop button.\"\"\"\n",
    "    global processing_thread, stop_event, audio_stream\n",
    "\n",
    "    status_label.value = \"Status: Stopping...\"\n",
    "    start_button.disabled = False\n",
    "    stop_button.disabled = True\n",
    "\n",
    "    # Signal the processing thread to stop\n",
    "    if processing_thread and processing_thread.is_alive():\n",
    "        stop_event.set()\n",
    "        # Wait briefly for the thread to finish\n",
    "        processing_thread.join(timeout=2.0)\n",
    "        if processing_thread.is_alive():\n",
    "             with output_area:\n",
    "                  print(\"Warning: Processing thread did not stop gracefully.\", file=sys.stderr)\n",
    "\n",
    "    # Stop and close the audio stream\n",
    "    if audio_stream:\n",
    "        try:\n",
    "            if audio_stream.active:\n",
    "                audio_stream.stop()\n",
    "            audio_stream.close()\n",
    "            with output_area:\n",
    "                 # Append stop message without clearing\n",
    "                 print(\"Audio stream stopped.\")\n",
    "        except Exception as e:\n",
    "            with output_area:\n",
    "                 print(f\"Error stopping audio stream: {e}\", file=sys.stderr)\n",
    "        finally:\n",
    "             audio_stream = None\n",
    "\n",
    "    status_label.value = \"Status: Idle\"\n",
    "    # Final cleanup of queue just in case\n",
    "    while not audio_queue.empty():\n",
    "        try: audio_queue.get_nowait()\n",
    "        except queue.Empty: break\n",
    "\n",
    "# Assign callbacks to buttons\n",
    "start_button.on_click(start_listening)\n",
    "stop_button.on_click(stop_listening)\n",
    "\n",
    "# Display Widgets\n",
    "controls = widgets.HBox([start_button, stop_button])\n",
    "display(controls, status_label, output_area)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes and Troubleshooting\n",
    "\n",
    "* **Microphone Access:** Ensure your terminal or Jupyter environment has permission to access the microphone in macOS System Settings > Privacy & Security > Microphone.\n",
    "* **Dependencies:** Double-check that all Python libraries (`mlx`, `mlx-whisper`, etc.) and system tools (`ffmpeg`, `portaudio`) are installed correctly in your environment.\n",
    "* **`mlx-transformers` API:** The code for loading and using the translation model (`load_translation_model`, `process_translation`) is based on common patterns but might need adjustments depending on the exact API of the `mlx-transformers` library version you install. Refer to its documentation and examples.\n",
    "* **Model Downloads:** The first time you run Cell 2 or use a specific model, it might take a while to download the model weights from Hugging Face Hub.\n",
    "* **Performance:** Real-time performance depends heavily on your Mac's specifications (M1, M2, M3, RAM) and the chosen model sizes. Smaller models (like `whisper-tiny` or `nllb-distilled-600M`) will be faster but potentially less accurate.\n",
    "* **VAD Tuning:** Adjust `VAD_THRESHOLD`, `MIN_SPEECH_DURATION_MS`, and `SILENCE_DURATION_MS_TRIGGER` in Cell 1 based on your microphone sensitivity and background noise for optimal speech segmentation.\n",
    "* **Errors:** Check the output area and your Jupyter console/terminal for error messages if the pipeline fails to start or run."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
